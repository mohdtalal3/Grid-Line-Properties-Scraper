{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 2\n",
      "Scraping page 2...\n",
      "Text file and Csv file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "def get_total_pages(soup):\n",
    "    paging = soup.find('ol', class_='paging')\n",
    "    if paging:\n",
    "        last_page = paging.find_all('li')[-2]  # Second to last <li> usually contains the last page number\n",
    "        return int(last_page.text.strip())\n",
    "    return 1  # If we can't find paging, assume there's only one page\n",
    "\n",
    "def scrape_page(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://www.gridlineproperties.com/\",\n",
    "        \"Sec-Ch-Ua\": '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": '\"Windows\"',\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    }\n",
    "\n",
    "    cookies = {\n",
    "        \"visitor_id923983\": \"1645789980\",\n",
    "        \"visitor_id923983-hash\": \"e8a533775eefe2838434db8df3ef98b09e0d16528f86cfbe49f298ee2fd36a8afe844717d15578498c5e61a7e3e1284cc07dcff8\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, cookies=cookies, timeout=30)\n",
    "        response.raise_for_status() \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find all listing boxes\n",
    "    listings = soup.find_all('article', class_='placard')\n",
    "\n",
    "    page_listings = []\n",
    "    for listing in listings:\n",
    "        url_element = listing.find('a', class_='placard-carousel-pseudo')\n",
    "        url = url_element['ng-href'] if url_element else 'URL not found'\n",
    "\n",
    "        address_element = listing.find('a', class_='subtitle-beta')\n",
    "        address = address_element.text.strip() if address_element else 'Address not found'\n",
    "\n",
    "        page_listings.append({'url': url, 'address': address})\n",
    "\n",
    "    return page_listings, soup\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://looplink.gridlineproperties.com/looplink/search/{}/?sk=234a143b4b437bf8508aa68ebcd723af&searchtype=fs\"\n",
    "\n",
    "# List of cities to check\n",
    "cities = [\n",
    "    'miami', 'north miami', 'hallandale', 'hollywood', 'fort lauderdale', 'hialeah', \n",
    "    'dania', 'davie', 'sunrise', 'plantation', 'pembroke pines', 'aventura', 'miramar', \n",
    "    'coral springs', 'north lauderdale', 'lauderdale lakes', 'boca raton', 'opa locka', \n",
    "    'weston', 'doral', 'kendall', 'homestead', 'coral gables'\n",
    "]\n",
    "\n",
    "all_listings = []\n",
    "matching_listings = []\n",
    "\n",
    "# Start with the first page\n",
    "first_page_listings, first_page_soup = scrape_page(base_url.format(1))\n",
    "all_listings.extend(first_page_listings)\n",
    "\n",
    "# Get total number of pages\n",
    "total_pages = get_total_pages(first_page_soup)\n",
    "print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "# Scrape remaining pages\n",
    "for page in range(2, total_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    page_listings, _ = scrape_page(base_url.format(page))\n",
    "    all_listings.extend(page_listings)\n",
    "    time.sleep(4)  # Add a delay to be respectful to the server\n",
    "\n",
    "# Process all listings\n",
    "for listing in all_listings:\n",
    "    if any(city.lower() in listing['address'].lower().replace(',', '').split() for city in cities):\n",
    "        matching_listings.append(listing)\n",
    "\n",
    "# Report on used and unused addresses\n",
    "used_addresses = set(listing['address'] for listing in matching_listings)\n",
    "unused_addresses = set(listing['address'] for listing in all_listings) - used_addresses\n",
    "\n",
    "df = pd.DataFrame(matching_listings)\n",
    "df.to_csv('grid_line_properties.csv', index=False)\n",
    "\n",
    "file_path = 'grid_line_properties addresses_matching_listings.txt'\n",
    "\n",
    "# Open the file in write mode and write the contents\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(\"Addresses Used:\\n\")\n",
    "    for address in used_addresses:\n",
    "        file.write(f\"{address}\\n\")\n",
    "    \n",
    "    file.write(\"\\nAddresses Not Used:\\n\")\n",
    "    for address in unused_addresses:\n",
    "        file.write(f\"{address}\\n\")\n",
    "    \n",
    "    file.write(f\"\\nTotal listings: {len(all_listings)}\\n\")\n",
    "    file.write(f\"Matching listings: {len(matching_listings)}\\n\")\n",
    "    file.write(f\"Non-matching listings: {len(all_listings) - len(matching_listings)}\\n\")\n",
    "\n",
    "print(f\"Text file and Csv file has been saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
